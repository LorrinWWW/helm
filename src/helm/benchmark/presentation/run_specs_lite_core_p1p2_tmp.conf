# HELM-lite `RunSpec`s for the benchmarking after v0.2.3.

entries: [
  ##### Question Answering #####

  # ## Reading comprehension ###
  # Scenarios: BoolQ, NarrativeQA, QuAC
  # {description: "boolq:model=mistralai/Mixtral-8x7B-Instruct-v0.1", priority: 1}
  # {description: "narrative_qa:model=mistralai/Mixtral-8x7B-Instruct-v0.1", priority: 2}
  # {description: "quac:model=mistralai/Mixtral-8x7B-Instruct-v0.1", priority: 1}

  # ### Reading comprehension and closedbook QA variants ###
  # # Scenarios: NaturalQuestions
  # {description: "natural_qa:model=mistralai/Mixtral-8x7B-Instruct-v0.1,mode=openbook_longans", priority: 1}
  # {description: "natural_qa:model=mistralai/Mixtral-8x7B-Instruct-v0.1,mode=closedbook", priority: 1}

  # ### Closed-book QA with multiple choice ###
  # # Scenarios: TruthfulQA, MMLU
  # {description: "truthful_qa:model=mistralai/Mixtral-8x7B-Instruct-v0.1,task=mc_single", priority: 1}

  # {description: "mmlu:model=mistralai/Mixtral-8x7B-Instruct-v0.1,subject=abstract_algebra", priority: 2}
  # {description: "mmlu:model=mistralai/Mixtral-8x7B-Instruct-v0.1,subject=college_chemistry", priority: 2}
  # {description: "mmlu:model=mistralai/Mixtral-8x7B-Instruct-v0.1,subject=computer_security", priority: 2}
  # {description: "mmlu:model=mistralai/Mixtral-8x7B-Instruct-v0.1,subject=econometrics", priority: 2}

  # ##### Sentiment Analysis #####

  # ## Scenarios: IMDB
  # {description: "imdb:model=mistralai/Mixtral-8x7B-Instruct-v0.1", priority: 1}

  # ##### (Miscellaneous) Text Classification #####

  # # Scenarios: RAFT
  # {description: "raft:subset=ade_corpus_v2,model=mistralai/Mixtral-8x7B-Instruct-v0.1", priority: 2}
  # {description: "raft:subset=banking_77,model=mistralai/Mixtral-8x7B-Instruct-v0.1", priority: 2}
  # {description: "raft:subset=neurips_impact_statement_risks,model=mistralai/Mixtral-8x7B-Instruct-v0.1", priority: 2}
  # {description: "raft:subset=one_stop_english,model=mistralai/Mixtral-8x7B-Instruct-v0.1", priority: 2}
  # {description: "raft:subset=overruling,model=mistralai/Mixtral-8x7B-Instruct-v0.1", priority: 2}
  # {description: "raft:subset=semiconductor_org_types,model=mistralai/Mixtral-8x7B-Instruct-v0.1", priority: 2}
  # {description: "raft:subset=tweet_eval_hate,model=mistralai/Mixtral-8x7B-Instruct-v0.1", priority: 2}
  # {description: "raft:subset=twitter_complaints,model=mistralai/Mixtral-8x7B-Instruct-v0.1", priority: 2}
  # {description: "raft:subset=systematic_review_inclusion,model=mistralai/Mixtral-8x7B-Instruct-v0.1", priority: 2}
  # {description: "raft:subset=tai_safety_research,model=mistralai/Mixtral-8x7B-Instruct-v0.1", priority: 2}
  # {description: "raft:subset=terms_of_service,model=mistralai/Mixtral-8x7B-Instruct-v0.1", priority: 2}

  # ##### Toxicity Detection #####

  # ## Scenarios: CivilComments
  # {description: "civil_comments:model=mistralai/Mixtral-8x7B-Instruct-v0.1,demographic=all", priority: 1}
  # {description: "civil_comments:model=mistralai/Mixtral-8x7B-Instruct-v0.1,demographic=male", priority: 2}
  # {description: "civil_comments:model=mistralai/Mixtral-8x7B-Instruct-v0.1,demographic=female", priority: 2}
  # {description: "civil_comments:model=mistralai/Mixtral-8x7B-Instruct-v0.1,demographic=LGBTQ", priority: 2}
  # {description: "civil_comments:model=mistralai/Mixtral-8x7B-Instruct-v0.1,demographic=christian", priority: 2}
  # {description: "civil_comments:model=mistralai/Mixtral-8x7B-Instruct-v0.1,demographic=muslim", priority: 2}
  # {description: "civil_comments:model=mistralai/Mixtral-8x7B-Instruct-v0.1,demographic=other_religions", priority: 2}
  # {description: "civil_comments:model=mistralai/Mixtral-8x7B-Instruct-v0.1,demographic=black", priority: 2}
  #{description: "civil_comments:model=mistralai/Mixtral-8x7B-Instruct-v0.1,demographic=white", priority: 2}
  {description: "twitter_aae:model=mistralai/Mixtral-8x7B-Instruct-v0.1,demographic=aa", priority: 1}
  #{description: "ice:model=mistralai/Mixtral-8x7B-Instruct-v0.1,gender=female", priority: 2}


]
